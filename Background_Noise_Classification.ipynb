{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c5888-a28e-40d7-84e8-8febfc6ee932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DATA PREPARATION\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the datasets from .h5 files created in the features extraction\n",
    "\n",
    "with h5py.File('UrbanSound8K_Spectrograms/Urban_spectograms.h5', 'r') as f:\n",
    "\n",
    "    spectrograms = f['spectrograms']\n",
    "    labels = f['labels']\n",
    "\n",
    "    \n",
    "    X_pretrain = spectrograms[()]\n",
    "    y_pretrain = labels[()]\n",
    "    \n",
    "    mask = np.any(X_pretrain != 0, axis=(1, 2))\n",
    "    X_pretrain = X_pretrain[mask]\n",
    "    y_pretrain = y_pretrain[mask]\n",
    "    \n",
    "with h5py.File('UrbanSound8K_Augmentation/Urban_spectograms_augmented.h5', 'r') as f:\n",
    "\n",
    "    spectrograms_aug = f['spectrograms_aug']\n",
    "    labels_aug = f['labels_aug']\n",
    "\n",
    "    \n",
    "    X_pretrain_aug = spectrograms_aug[()]\n",
    "    y_pretrain_aug = labels_aug[()]\n",
    "    \n",
    "    mask_aug = np.any(X_pretrain_aug != 0, axis=(1, 2))\n",
    "    X_pretrain_aug = X_pretrain_aug[mask_aug]\n",
    "    y_pretrain_aug = y_pretrain_aug[mask_aug]\n",
    "\n",
    "with h5py.File('ybss-spectrograms/Ybss-spectrograms.h5', 'r') as f:\n",
    "    \n",
    "    test_spec = f['test_spec']\n",
    "    test_labs = f['test_labs']\n",
    "\n",
    "    \n",
    "    X_test = test_spec[()]\n",
    "    y_test = test_labs[()]\n",
    "    \n",
    "    \n",
    "    train_spec = f['train_spec']\n",
    "    train_labs = f['train_labs']\n",
    "\n",
    "    \n",
    "    X_train = train_spec[()]\n",
    "    y_train = train_labs[()]\n",
    "    \n",
    "with h5py.File('final_test_dataset_0.4/final_dataset_0.4.h5', 'r') as f:\n",
    "\n",
    "    spectrograms_mix_04 = f['spectrograms_mix']\n",
    "    labels_mix_04 = f['labels_mix']\n",
    "\n",
    "    \n",
    "    X_final_04 = spectrograms_mix_04[()]\n",
    "    y_final_04 = labels_mix_04[()]\n",
    "    \n",
    "    mask_final_04 = np.any(X_final_04 != 0, axis=(1, 2))\n",
    "    X_final_04 = X_final_04[mask_final_04]\n",
    "    y_final_04 = y_final_04[mask_final_04]\n",
    "    \n",
    "with h5py.File('final_test_dataset_0.3/final_dataset_0.3.h5', 'r') as f:\n",
    "\n",
    "    spectrograms_mix_03 = f['spectrograms_mix']\n",
    "    labels_mix_03 = f['labels_mix']\n",
    "\n",
    "    \n",
    "    X_final_03 = spectrograms_mix_03[()]\n",
    "    y_final_03 = labels_mix_03[()]\n",
    "    \n",
    "    mask_final_03 = np.any(X_final_03 != 0, axis=(1, 2))\n",
    "    X_final_03 = X_final_03[mask_final_03]\n",
    "    y_final_03 = y_final_03[mask_final_03]\n",
    "    \n",
    "    \n",
    "with h5py.File('final_test_dataset_0.2/final_dataset_0.2.h5', 'r') as f:\n",
    "\n",
    "    spectrograms_mix_02 = f['spectrograms_mix']\n",
    "    labels_mix_02 = f['labels_mix']\n",
    "\n",
    "    \n",
    "    X_final_02 = spectrograms_mix_02[()]\n",
    "    y_final_02 = labels_mix_02[()]\n",
    "    \n",
    "    mask_final_02 = np.any(X_final_02 != 0, axis=(1, 2))\n",
    "    X_final_02 = X_final_02[mask_final_02]\n",
    "    y_final_02 = y_final_02[mask_final_02]\n",
    "    \n",
    "with h5py.File('final_test_dataset_0.1/final_dataset_0.1.h5', 'r') as f:\n",
    "\n",
    "    spectrograms_mix_01 = f['spectrograms_mix']\n",
    "    labels_mix_01 = f['labels_mix']\n",
    "\n",
    "    \n",
    "    X_final_01 = spectrograms_mix_01[()]\n",
    "    y_final_01 = labels_mix_01[()]\n",
    "    \n",
    "    mask_final_01 = np.any(X_final_01 != 0, axis=(1, 2))\n",
    "    X_final_01 = X_final_01[mask_final_01]\n",
    "    y_final_01 = y_final_01[mask_final_01]\n",
    "    \n",
    "with h5py.File('final_test_dataset_0.05/final_dataset_0.05.h5', 'r') as f:\n",
    "\n",
    "    spectrograms_mix_005 = f['spectrograms_mix']\n",
    "    labels_mix_005 = f['labels_mix']\n",
    "\n",
    "    \n",
    "    X_final_005 = spectrograms_mix_005[()]\n",
    "    y_final_005 = labels_mix_005[()]\n",
    "    \n",
    "    mask_final_005 = np.any(X_final_005 != 0, axis=(1, 2))\n",
    "    X_final_005 = X_final_005[mask_final_005]\n",
    "    y_final_005 = y_final_005[mask_final_005]\n",
    "\n",
    "\n",
    "# One-hot encode the target variables to feed it to the neural network.\n",
    "\n",
    "y_pretrain = to_categorical(y_pretrain)\n",
    "y_pretrain_aug = to_categorical(y_pretrain_aug)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)\n",
    "y_final_04 = to_categorical(y_final_04)\n",
    "y_final_03 = to_categorical(y_final_03)\n",
    "y_final_02 = to_categorical(y_final_02)\n",
    "y_final_01 = to_categorical(y_final_01)\n",
    "y_final_005 = to_categorical(y_final_005)\n",
    "\n",
    "# We do train-validation split\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15397c-7a51-46cc-b4bd-2ac221bf6e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### We build the CNN model (Without PreTrain)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, GlobalMaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parallelize the taks through all available GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    ## Create the model\n",
    "    \n",
    "    model = Sequential()\n",
    "    ## Add the model layers\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(127,64,1), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Last max pool layer\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    # Fully connected layer and Output layer with softmax activation function\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## Compile the model using accuracy to measure model performance, corossentropy as loss measure and adam as optimizer.\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## Train the model\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data = (X_validate,y_validate), epochs=250, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9012b62-21f2-4007-af1b-8abb5916f957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Build the CNN model (With PreTrain using UrbanSound8K dataset USING KFOLD IN PRETRAINING)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, GlobalMaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "#Pre train\n",
    "# Parallelize the taks through all available GPUs\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    ## Create model\n",
    "    pretrain = Sequential()\n",
    "    ## Add model layers\n",
    "    # Block 1\n",
    "    pretrain.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(127,64,1), padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    pretrain.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    pretrain.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    pretrain.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    pretrain.add(BatchNormalization())\n",
    "    pretrain.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Last max pool layer\n",
    "    pretrain.add(GlobalMaxPooling2D())\n",
    "\n",
    "    # Fully connected layer and Output layer with softmax activation function\n",
    "    pretrain.add(Flatten())\n",
    "    pretrain.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "pretrain.summary()\n",
    "pretrain.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Define the number of folds for cross-validation\n",
    "\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Train and evaluate the model using 10-fold cross-validation\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_pretrain_aug, y_pretrain_aug)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    train_x, val_x = X_pretrain_aug[train_idx], X_pretrain_aug[val_idx]\n",
    "    train_y, val_y = y_pretrain_aug[train_idx], y_pretrain_aug[val_idx]\n",
    "    hist_pretrain = pretrain.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=10, batch_size=64)\n",
    "    \n",
    "\n",
    "pretrain.save('my_lastest_model.h5')\n",
    "\n",
    "# Classify the final dataset\n",
    "# Parallelize the taks through all available GPUs\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    ## Create model\n",
    "    \n",
    "    model = Sequential()\n",
    "    ## Add model layers\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(127,64,1), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Last max pool layer\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    # Fully connected layer and Output layer with softmax activation function\n",
    "    model.add(Flatten())\n",
    "    model.load_weights('my_lastest_model.h5', by_name=True)\n",
    "    model.add(Dense(10, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    \n",
    "\n",
    "model.summary()\n",
    "# Freeze all convolutional layers\n",
    "#for layer in model.layers:\n",
    "#    if 'conv' in layer.name:\n",
    "#        layer.trainable = False\n",
    "    \n",
    "trainable_layers = [layer for layer in model.layers if layer.trainable]\n",
    "\n",
    "# Count the number of frozen layers\n",
    "\n",
    "num_frozen_layers = len(model.layers) - len(trainable_layers)\n",
    "\n",
    "print(\"Number of frozen layers: \", num_frozen_layers)\n",
    "\n",
    "# We randomly initialize the weights of the classification part\n",
    "\n",
    "adam_final = Adam(learning_rate=0.00001)\n",
    "SGD_final = SGD(learning_rate=0.00001)\n",
    "model.compile(optimizer=adam_final, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_validate, y_validate), epochs=150, batch_size=64)\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae84cf7-5a8d-4239-af8b-835ed2338bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Build and train the CNN model (After PreTrain using UrbanSound8K dataset and .h5 file with pretrain weights)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, GlobalMaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# Classify the final dataset\n",
    "# Parallelize the taks through all available GPUs\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    ## Create model\n",
    "    \n",
    "    model = Sequential()\n",
    "    ## Add model layers\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(127,64,1), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # Last max pool layer\n",
    "    \n",
    "    model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    # Fully connected layer and Output layer with softmax activation function\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.load_weights('my_lastest_model.h5', by_name=True)\n",
    "    #model.add(Dense(units=512,activation=\"relu\"))\n",
    "    #model.add(Dense(units=256,activation=\"relu\"))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "\n",
    "model.summary()\n",
    "# Freeze all convolutional layers\n",
    "#for layer in model.layers:\n",
    "#    if 'conv' in layer.name:\n",
    "#        layer.trainable = False\n",
    "    \n",
    "trainable_layers = [layer for layer in model.layers if layer.trainable]\n",
    "\n",
    "# Count the number of frozen layers\n",
    "num_frozen_layers = len(model.layers) - len(trainable_layers)\n",
    "\n",
    "print(\"Number of frozen layers: \", num_frozen_layers)\n",
    "\n",
    "# We randomly initialize the weights of the classification part\n",
    "\n",
    "adam_final = Adam(learning_rate=0.00001)\n",
    "SGD_final = SGD(learning_rate=0.00001)\n",
    "model.compile(optimizer=adam_final, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_validate, y_validate), epochs=110, batch_size=64)\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc470b9c-5ab5-4b68-a78c-a34c608f9521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## We show the final results\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# We print the final accuracy and loss for the test set.\n",
    "val_acc = hist.history['val_accuracy'][-1]\n",
    "val_loss = hist.history['val_loss'][-1]\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(val_acc * 100))\n",
    "print(\"Validation loss: {:.2f}\".format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b1c40-2666-4053-a1ef-8900941298e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "conf=sns.heatmap(cm, annot=True, fmt='d')\n",
    "conf.set_xticklabels(['auto_rikshaw','cricket_crowd','electronic_stapler','formula_1','grass_cutting','guitar','helicoptor','sewing_machine','tap_water','traffic'])\n",
    "conf.set_yticklabels(['auto_rikshaw','cricket_crowd','electronic_stapler','formula_1','grass_cutting','guitar','helicoptor','sewing_machine','tap_water','traffic'])\n",
    "\n",
    "# We print the final accuracy and loss for the test set.\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "print(\"Testing loss: {:.2f}\".format(test_loss))\n",
    "\n",
    "# Customize the plot,1,1\n",
    "plt.xticks(fontsize=9)\n",
    "plt.yticks(fontsize=9)\n",
    "plt.xticks(rotation=80)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589d631-1d9f-4ca5-a3a7-46c4a76b5ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_final_04).argmax(axis=-1)\n",
    "y_true = y_final_04.argmax(axis=-1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "conf=sns.heatmap(cm, annot=True, fmt='d')\n",
    "conf.set_xticklabels(['auto_rikshaw','cricket_crowd','electronic_stapler','formula_1','grass_cutting','guitar','helicoptor','sewing_machine','tap_water','traffic'])\n",
    "conf.set_yticklabels(['auto_rikshaw','cricket_crowd','electronic_stapler','formula_1','grass_cutting','guitar','helicoptor','sewing_machine','tap_water','traffic'])\n",
    "\n",
    "# We print the final accuracy and loss for the test set.\n",
    "test_loss, test_accuracy = model.evaluate(X_final_04, y_final_04, verbose=0)\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "print(\"Testing loss: {:.2f}\".format(test_loss))\n",
    "\n",
    "# Customize the plot,1,1\n",
    "plt.xticks(fontsize=9)\n",
    "plt.yticks(fontsize=9)\n",
    "plt.xticks(rotation=80)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bde70-7c75-4915-8505-c9823cee7ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We draw the ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_pred_proba = model.predict(X_final_005)\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(10):\n",
    "    fpr[i], tpr[i], thresholds = roc_curve(y_final_005[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "for i in range(10):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC={roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
